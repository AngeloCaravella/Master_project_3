\babel@toc {english}{}\relax 
\babel@toc {italian}{}\relax 
\babel@toc {english}{}\relax 
\contentsline {chapter}{List of Acronyms}{4}{section*.4}%
\contentsline {chapter}{\numberline {1}Introduction}{7}{chapter.1}%
\contentsline {subsection}{\numberline {1.0.1}Background and Relevance of Electric Vehicles and Vehicle-to-Grid}{7}{subsection.1.0.1}%
\contentsline {subsection}{\numberline {1.0.2}Challenges in EV Integration into the Electricity Grid and the Role of Artificial Intelligence}{8}{subsection.1.0.2}%
\contentsline {subsection}{\numberline {1.0.3}Objectives and Contributions of the Thesis}{9}{subsection.1.0.3}%
\contentsline {subsection}{\numberline {1.0.4}Thesis Structure}{10}{subsection.1.0.4}%
\contentsline {chapter}{\numberline {2}State of the Art in Optimal V2G Management}{11}{chapter.2}%
\contentsline {section}{\numberline {2.1}The V2G Imperative: A Cornerstone of Europe's Green Transition}{11}{section.2.1}%
\contentsline {section}{\numberline {2.2}The Optimizer's Trilemma: Navigating a Stochastic World}{13}{section.2.2}%
\contentsline {section}{\numberline {2.3}A New Paradigm for Control: Reinforcement Learning}{13}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}The Language of Learning: Markov Decision Processes (MDPs)}{13}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Judging the Future: Value Functions and Actor-Critic Architectures}{14}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Advanced Reward Engineering}{14}{subsection.2.3.3}%
\contentsline {subsubsection}{A Taxonomy of Reward Shaping Techniques}{15}{section*.5}%
\contentsline {subsubsection}{Most Utilized Techniques in Reinforcement Learning for V2G}{16}{section*.6}%
\contentsline {section}{\numberline {2.4}The Rise of Deep Reinforcement Learning for V2G Control}{17}{section.2.4}%
\contentsline {section}{\numberline {2.5}A Comparative Perspective on Control Methodologies}{18}{section.2.5}%
\contentsline {chapter}{\numberline {3}An Enhanced V2G Simulation Framework for Robust Control}{20}{chapter.3}%
\contentsline {section}{\numberline {3.1}Core Simulator Architecture}{20}{section.3.1}%
\contentsline {section}{\numberline {3.2}Core Physical Models}{21}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}EV Model and Charging/Discharging Dynamics}{21}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Battery Degradation Model}{21}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}EV Behavior and Grid Models}{22}{subsection.3.2.3}%
\contentsline {section}{\numberline {3.3}A Dual-Pronged Evaluation Architecture}{22}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Single-Domain Specialization}{22}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Multi-Scenario Generalization}{22}{subsection.3.3.2}%
\contentsline {section}{\numberline {3.4}Software and Experimentation Workflow}{23}{section.3.4}%
\contentsline {section}{\numberline {3.5}Evaluation Metrics}{23}{section.3.5}%
\contentsline {section}{\numberline {3.6}Reinforcement Learning Formulation}{24}{section.3.6}%
\contentsline {subsection}{\numberline {3.6.1}State Space ($S$)}{24}{subsection.3.6.1}%
\contentsline {subsection}{\numberline {3.6.2}Action Space ($A$)}{25}{subsection.3.6.2}%
\contentsline {subsection}{\numberline {3.6.3}Reward Function ($R(s, a, s')$)}{25}{subsection.3.6.3}%
\contentsline {subsection}{\numberline {3.6.4}A History-Based Adaptive Reward for Profit Maximization}{26}{subsection.3.6.4}%
\contentsline {subsubsection}{Economic Profit}{26}{section*.7}%
\contentsline {subsubsection}{Adaptive User Satisfaction Penalty}{26}{section*.8}%
\contentsline {subsubsection}{Adaptive Transformer Overload Penalty}{27}{section*.9}%
\contentsline {subsubsection}{Rationale and Significance}{27}{section*.10}%
\contentsline {section}{\numberline {3.7}Model Predictive Control (MPC)}{28}{section.3.7}%
\contentsline {subsection}{\numberline {3.7.1}System Model}{28}{subsection.3.7.1}%
\contentsline {subsection}{\numberline {3.7.2}Optimization Problem}{28}{subsection.3.7.2}%
\contentsline {section}{\numberline {3.8}Offline Optimization with Gurobi}{28}{section.3.8}%
\contentsline {subsection}{\numberline {3.8.1}Decision Variables}{28}{subsection.3.8.1}%
\contentsline {subsection}{\numberline {3.8.2}Objective Function (Example: Profit Maximization)}{29}{subsection.3.8.2}%
\contentsline {subsection}{\numberline {3.8.3}Main Constraints}{29}{subsection.3.8.3}%
\contentsline {section}{\numberline {3.9}Online MPC Formulation (PuLP Implementation)}{29}{section.3.9}%
\contentsline {subsection}{\numberline {3.9.1}Mathematical Formulation}{29}{subsection.3.9.1}%
\contentsline {subsubsection}{Objective Function: Net Operational Profit}{30}{section*.11}%
\contentsline {subsubsection}{System Constraints}{30}{section*.12}%
\contentsline {paragraph}{Energy Balance Dynamics.}{30}{section*.13}%
\contentsline {paragraph}{Power Limits and Mutual Exclusion.}{30}{section*.14}%
\contentsline {paragraph}{State of Energy (SoE) Limits.}{30}{section*.15}%
\contentsline {paragraph}{User Satisfaction (Hard Constraint).}{31}{section*.16}%
\contentsline {paragraph}{Transformer Power Limit.}{31}{section*.17}%
\contentsline {section}{\numberline {3.10}Conceptual Comparison: PuLP MPC vs. Gurobi Offline Optimizer}{31}{section.3.10}%
\contentsline {subsection}{\numberline {3.10.1}Core Philosophy: Controller vs. Judge}{31}{subsection.3.10.1}%
\contentsline {subsection}{\numberline {3.10.2}Objective Function: Operational Profit vs. Energy Arbitrage}{31}{subsection.3.10.2}%
\contentsline {subsection}{\numberline {3.10.3}Handling of User Satisfaction: Hard vs. Soft Constraints}{32}{subsection.3.10.3}%
