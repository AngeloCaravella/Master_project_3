\section{Online MPC Formulation (PuLP Implementation)}

The Model Predictive Control (MPC) implemented with PuLP solves a profit maximization problem at each time step $t$ over a finite prediction horizon $H$. This formulation is designed for online, real-time control, where decisions are made based on the current system state and future predictions.

\subsection{Mathematical Formulation}
At each time step $t$, the MPC controller solves the following optimization problem.

\subsubsection{Objective Function: Net Operational Profit}
The objective is to maximize the total net operational profit over the control horizon $H$. This provides a comprehensive economic model that goes beyond simple energy arbitrage.
\begin{equation}
\max_{P^{\text{ch}}, P^{\text{dis}}, z} \sum_{k=t}^{t+H-1} \sum_{i \in \text{CS}} \left( \text{Revenues}_{i,k} - \text{Costs}_{i,k} \right)
\end{equation}
The revenue and cost components are defined for each station $i$ at time step $k$ as:
\begin{itemize}
    \item \textbf{Revenues} consist of:
    \begin{itemize}
        \item Grid Sales Revenue (V2G): $c^{\text{sell}}_k \cdot P^{\text{dis}}_{i,k} \cdot \Delta t$
        \item User Charging Revenue: $c^{\text{user}} \cdot P^{\text{ch}}_{i,k} \cdot \Delta t$
    \end{itemize}
    \item \textbf{Costs} consist of:
    \begin{itemize}
        \item Grid Purchase Cost: $c^{\text{buy}}_k \cdot P^{\text{ch}}_{i,k} \cdot \Delta t$
        \item Battery Degradation Cost: $c^{\text{deg}} \cdot (P^{\text{ch}}_{i,k} + P^{\text{dis}}_{i,k}) \cdot \Delta t$
    \end{itemize}
\end{itemize}
where $c^{\text{sell}}_k$ and $c^{\text{buy}}_k$ are the time-varying electricity prices, $c^{\text{user}}$ is the fixed price for the end-user, $c^{\text{deg}}$ is the estimated cost of battery degradation per kWh cycled, and $\Delta t$ is the time step duration.

\subsubsection{System Constraints}
The optimization is subject to the following constraints for each station $i$ and time step $k \in [t, t+H-1]$.

\paragraph{Energy Balance Dynamics.} The state of energy of the EV battery evolves according to:
\begin{equation}
E_{i,k} = E_{i,k-1} + \left( \eta^{\text{ch}} P^{\text{ch}}_{i,k} - \frac{1}{\eta^{\text{dis}}} P^{\text{dis}}_{i,k} \right) \cdot \Delta t
\end{equation}
where the initial state $E_{i,t-1}$ is the currently measured energy level of the EV.

\paragraph{Power Limits and Mutual Exclusion.} Charging and discharging powers are bounded by the EV's capabilities and controlled by a binary variable $z_{i,k}$ to prevent simultaneous operation.
\begin{align}
    0 &\le P^{\text{ch}}_{i,k} \le P^{\text{ch,max}}_{i} \cdot z_{i,k} \\
    0 &\le P^{\text{dis}}_{i,k} \le P^{\text{dis,max}}_{i} \cdot (1 - z_{i,k})
\end{align}

\paragraph{State of Energy (SoE) Limits.} The battery energy level must remain within its physical operational window.
\begin{equation}
E^{\text{min}}_{i} \le E_{i,k} \le E^{\text{max}}_{i}
\end{equation}

\paragraph{User Satisfaction (Hard Constraint).} The desired energy level must be met at the time of departure. This is modeled as a hard constraint, reflecting a non-negotiable service requirement.
\begin{equation}
E_{i,k_{\text{dep}}} \ge E^{\text{des}}_{i}
\end{equation}
where $k_{\text{dep}}$ is the predicted departure step of the EV within the horizon.

\paragraph{Transformer Power Limit.} The total net power drawn from (or injected into) the grid by all charging stations must not exceed the transformer's maximum capacity.
\begin{equation}
\sum_{i \in \text{CS}} (P^{\text{ch}}_{i,k} - P^{\text{dis}}_{i,k}) \le P^{\text{tr,max}}
\end{equation}


\section{Approximate Explicit MPC: A Machine Learning Approach}
The online, implicit MPC formulation provides high-quality control decisions by solving an optimization problem at every time step. However, this approach has a significant drawback: its computational complexity. For scenarios with a large number of EVs or a long control horizon, solving a Mixed-Integer Linear Program (MILP) in real-time can be prohibitively slow, making it impractical for many real-world applications.

To overcome this limitation, this work implements an \textbf{Approximate Explicit Model Predictive Controller (A-MPC)}. This controller leverages machine learning to replace the computationally expensive online optimization with a fast, lightweight inference step.

\subsection{Methodology: From Oracle to Apprentice}
The core idea is to treat the slow but powerful online MPC as an "oracle" or expert teacher. An apprentice model, in this case a \texttt{RandomForestRegressor} from the \texttt{scikit-learn} library, is trained to mimic the oracle's behavior. The process is as follows:
\begin{enumerate}
    \item \textbf{Data Generation:} The online MPC is run across a diverse range of simulated scenarios. At each step, the state of the environment and the corresponding optimal action computed by the MPC are recorded. This creates a large dataset of state-action pairs, where the actions are considered to be the "ground truth" optimal decisions.
    \item \textbf{State Vector Formulation:} The state $s_t$ fed to the machine learning model is a carefully crafted vector that summarizes all necessary information for making a control decision. It is a fixed-size vector composed of:
    \begin{equation}
        s_t = [ \mathbf{SoC}, \mathbf{T}^{\text{rem}}, \mathbf{C}^{\text{ch}}, \mathbf{C}^{\text{dis}} ]^T
    \end{equation}
    where:
    \begin{itemize}
        \item $\mathbf{SoC}$: A vector of the current State of Charge for all charging stations (padded to a maximum size).
        \item $\mathbf{T}^{\text{rem}}$: A vector of the remaining time until departure for each connected EV.
        \item $\mathbf{C}^{\text{ch}}$: A vector of predicted future charging prices over the horizon $H$.
        \item $\mathbf{C}^{\text{dis}}$: A vector of predicted future discharging prices over the horizon $H$.
    \end{itemize}
    \item \textbf{Offline Training:} The \texttt{RandomForestRegressor} model, denoted $f_{\theta}$, is trained offline on this dataset to learn the mapping from a given state $s_t$ to the oracle's action $a_t$. The model's parameters $\theta$ are optimized to minimize the difference between its predicted action and the oracle's action.
    \item \textbf{Online Inference:} Once trained, the A-MPC controller can be deployed. At each time step, it simply constructs the state vector $s_t$ and computes the action via a fast forward pass through the trained model:
    \begin{equation}
        a_t = f_{\theta}(s_t)
    \end{equation}
    This inference step is orders of magnitude faster than solving a MILP, enabling real-time control for large-scale systems.
\end{enumerate}

\section{Lyapunov-based Adaptive Horizon MPC}
While the A-MPC offers a significant speed-up, it is an approximation and may not always match the performance of the fully-fledged online MPC. A second enhancement developed in this work is the \textbf{Lyapunov-based Adaptive Horizon MPC}, which aims to reduce the computational burden of the online MPC while retaining its optimality and stability guarantees. This method represents an essential improvement, creating an intelligent trade-off between computational cost and control performance.

\subsection{Core Concept: Dynamic Horizon Adjustment}
The key insight is that a long prediction horizon is not always necessary. When the system is in a stable state and far from its operational constraints, a shorter horizon is sufficient for making good decisions. Conversely, when the system is in a complex or critical state (e.g., an EV is close to its departure time but has a low SoC), a longer horizon is needed for careful planning.

This adaptive controller dynamically adjusts its prediction horizon $H_t$ at each step based on the stability of the system, which is formally assessed using a Lyapunov function.

\subsection{Lyapunov Stability for V2G Control}
A Lyapunov function $V(x)$ is a scalar function that can be thought of as a measure of the system's "energy" or deviation from a desired equilibrium state. For the V2G system, we define the state as the vector of energy levels of all connected EVs, $E = [E_1, E_2, \dots, E_N]^T$. The desired state is the vector of desired energy levels at departure, $E^{\text{des}}$. The Lyapunov function is defined as the sum of the squared errors from this desired state:
\begin{equation}
    V(E) = \sum_{i \in \text{EVs}} (E_i - E_i^{\text{des}})^2
\end{equation}
For the system to be stable, the value of this function must decrease at each step, ensuring the system is always progressing towards its goal. This is known as the Lyapunov decrease condition:
\begin{equation}
    V(E_{t+1}) \le V(E_t) - \alpha V(E_t)
\end{equation}
where $E_{t+1}$ is the state at the next time step resulting from the current control action, and $\alpha$ is a small positive constant that sets the minimum required rate of convergence.

\subsection{Horizon Shortening and Extension}
The adaptive MPC algorithm uses this stability condition to govern its horizon length. At each time step $t$:
\begin{enumerate}
    \item The MPC solves the optimization problem using its current horizon, $H_t$.
    \item It calculates the predicted next state $E_{t+1}$ based on the computed optimal action.
    \item It checks if the Lyapunov decrease condition is satisfied.
    \begin{itemize}
        \item \textbf{If Stable:} The condition holds. The controller is performing well. We can afford to reduce the computational load for the next step by shortening the horizon:
        \begin{equation}
            H_{t+1} = \max(H_{\min}, H_t - 1)
        \end{equation}
        \item \textbf{If Not Stable:} The condition is violated. The system requires more careful planning. The horizon for the next step is extended to provide a longer view into the future:
        \begin{equation}
            H_{t+1} = \min(H_{\max}, H_t + 1)
        \end{equation}
    \end{itemize}
\end{enumerate}
This intelligent adjustment makes the online MPC more efficient and practical, reducing computation time during stable periods while retaining the ability to perform deep planning when necessary.