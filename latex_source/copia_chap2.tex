\chapter{State of the Art in Optimal V2G Management}

\section{The V2G Imperative: A Cornerstone of Europe's Green Transition}
Society finds itself at a critical juncture, grappling with the twin revolutions of decarbonizing transport and reshaping our energy systems. This is not merely an ambition but a legally binding mandate, enshrined in frameworks like the \textbf{European Green Deal} and its ambitious \textbf{"Fit for 55"} package \footcite{european_commission_2021_fit_for_55}. These policies impose a rapid phase-out of internal combustion engines and demand a massive scale-up of renewable energy sources, as detailed in the revised Renewable Energy Directive (RED III). At the very nexus of this challenge lies the proliferation of Electric Vehicles (EVs).
\\
\noindent
Initially viewed with apprehension—a looming threat of massive, synchronized loads poised to destabilize fragile distribution networks—that perception is now obsolete. Today, EVs must be seen not as a problem, but as a foundational pillar of the solution. This change in perspective is embodied in the concept of \textbf{Vehicle-to-Grid (V2G)}. V2G acts as the critical enabling technology that can transform millions of EVs from passive consumers into active, distributed, and intelligent assets for the grid. The key lies hidden in plain sight: private vehicles remain parked and connected for an astonishing 96\% of their existence \footcite{evertsson2024investigating}, representing a potential of terawatt-hours of mobile storage waiting to be harnessed.
\\
\noindent
The true power of V2G emerges not from the individual, but from the collective. A single EV's contribution is a whisper, but a coordinated fleet, managed by an aggregator, becomes a roar—a \textbf{Virtual Power Plant (VPP)}. This collective entity, with the lightning-fast response of battery inverters, can deliver a spectrum of critical services. This capability is the linchpin for stabilizing a grid increasingly reliant on the fluctuating whims of wind and sun, making the high renewable penetration targets of the EU feasible. The services unlocked by this capability are foundational to building the smart, resilient grid of tomorrow:
\begin{itemize}
    \item \textbf{Frequency Regulation:} The grid's heartbeat. V2G fleets can inject or absorb power in seconds, instantly counteracting supply-demand imbalances to maintain the stable 50/60 Hz frequency, preventing cascading failures and blackouts \footcite{alfaverh2022optima, sadeghi2021deep}.
    
    \item \textbf{Demand Response and Peak Shaving:} By intelligently shifting charging to off-peak hours and discharging during peak demand, V2G flattens the load curve. This reduces our reliance on expensive and polluting "peaker" plants and can defer trillions in grid infrastructure upgrades \footcite{orfanoudakis2022deep}.
    
    \item \textbf{Renewable Energy Integration:} Perhaps its most profound impact, V2G fleets can act as a giant, distributed sponge, absorbing surplus solar and wind energy that would otherwise be curtailed, and releasing it when the sun sets or the wind dies down. This directly supports the integration goals of RED III and mitigates intermittency \footcite{khan2024review, zou2021deep}.
\end{itemize}
This vision is no longer a distant prospect; it is actively being codified into European law and technical standards. The landmark \textbf{Alternative Fuels Infrastructure Regulation (AFIR, EU 2023/1804)} now mandates that new public charging infrastructure must support smart and bidirectional charging. This legal requirement is given its technical teeth by specific standards, as a delegated regulation specifies that from 2027, charging points must comply with \textbf{ISO 15118-20}—a standard that explicitly defines the communication protocols for bidirectional power transfer. This regulatory push is complemented by large-scale pilot projects like \textbf{'SCALE'} and \textbf{'V2G Balearic Islands'}, which are testing the technology's technical and economic viability on an industrial scale.
\\
\noindent
However, while the regulatory foundation is being laid, significant barriers to widespread adoption remain, creating a complex landscape that technology and policy must navigate together. Key challenges include:
\begin{itemize}
    \item \textbf{Market and Economic Hurdles:} A clear, pan-European framework for remunerating EV owners for grid services is still absent. Critical issues like the \textbf{"double taxation"} of electricity—taxed both on charging and discharging—create significant economic disincentives that must be resolved.
    \item \textbf{Regulatory and Grid Access Rules:} The role of EV fleets as a flexibility resource is not yet uniformly recognized in electricity markets. Standardized procedures for grid connection, aggregator certification, and secure data exchange are still under development, hindering market access.
    \item \textbf{Technical and Consumer Barriers:} On the consumer side, concerns about accelerated \textbf{battery degradation} and its impact on vehicle warranties remain a primary obstacle. Furthermore, the reality is that not all EVs or chargers are currently equipped with the necessary hardware and software to support V2G.
\end{itemize}
The central challenge, which is the focus of this thesis, is therefore not simply about enabling V2G, but about orchestrating it \textit{intelligently}. This requires a control strategy sophisticated enough to operate within this nascent regulatory framework, navigate its economic uncertainties, and overcome technical constraints to unlock the immense potential of EVs as a cornerstone of a sustainable energy future.

%%%%%%%%%%%%%%%%%
\section{The Optimizer's Trilemma: Navigating a Stochastic World}
While the potential is immense, orchestrating this symphony of distributed assets presents a formidable challenge. An aggregator's primary driver is economic viability, but pursuing profit in isolation is a recipe for failure. Optimal V2G management is a delicate balancing act, a genuine multi-objective optimization problem often framed as the "V2G trilemma": the simultaneous pursuit of \textbf{economic profitability}, the preservation of \textbf{battery longevity}, and the guarantee of \textbf{user convenience}.
\\
\noindent
This is not a simple trade-off but a dynamic problem steeped in \textbf{stochasticity} and \textbf{uncertainty} from multiple sources:
\begin{itemize}
    \item \textbf{Market Volatility:} Electricity prices can fluctuate wildly based on unpredictable supply and demand.
    \item \textbf{Renewable Intermittency:} The output of solar and wind generation is inherently variable.
    \item \textbf{Human Behavior:} EV owners' arrival times, departure times, and energy needs are not deterministic; a driver might need to leave unexpectedly, a non-negotiable constraint that any intelligent system must respect.
\end{itemize}
Such a chaotic environment renders static, rule-based control systems obsolete. We need an approach that can learn, adapt, and make intelligent decisions in real-time under profound uncertainty. This is precisely the domain where Reinforcement Learning excels.

\section{The Economics of V2G: Navigating Energy Markets}
\label{sec:v2g_economics}
The economic viability of many Vehicle-to-Grid services, particularly energy arbitrage, hinges on the price differential between buying and selling electricity. Understanding the structure of energy markets and the composition of electricity prices is therefore essential for any realistic V2G analysis.

\subsection{Sources for Energy Price Data}
The most relevant prices for V2G optimization are those from the wholesale electricity markets, where energy is traded in bulk before reaching the end consumer. In Europe, the primary markets are:
\begin{itemize}
    \item \textbf{Day-Ahead Market:} This is an auction-based market where prices and volumes are determined for every hour of the following day. This is the most common signal used for planning charging and discharging schedules.
    \item \textbf{Intraday Market:} This market operates closer to real-time, allowing participants to adjust their positions based on updated forecasts (e.g., for renewable generation).
\end{itemize}
Access to this data is crucial for both simulation and real-world deployment. Key public sources for European market data include:
\begin{itemize}
    \item \textbf{ENTSO-E Transparency Platform:} A mandatory, open-access platform that provides a vast repository of pan-European electricity market data, including harmonized day-ahead prices for numerous countries. It is a primary source for academic research and is accessible via a web portal and a free RESTful API.
    \item \textbf{National Transmission System Operators (TSOs):} Many national TSOs (e.g., Terna in Italy, RTE in France) publish detailed market data for their respective regions.
    \item \textbf{Power Exchanges:} Exchanges like \textbf{EPEX SPOT} are where the actual trading occurs. While they are the direct source of price data, comprehensive and real-time access is typically part of a commercial subscription service.
\end{itemize}

\subsection{Buying vs. Selling: The Retail-Wholesale Spread}
A critical distinction for V2G economics is the difference between the price an EV owner pays to charge their vehicle versus the price an aggregator receives for selling that energy back to the grid.

\begin{itemize}
    \item \textbf{Selling Price (V2G Revenue):} When an EV provides energy to the grid, the revenue is based on the \textbf{wholesale price} (e.g., the day-ahead spot price). This price reflects the pure cost of energy at a specific time.
    
    \item \textbf{Buying Price (Charging Cost):} The price paid by an end consumer to charge their EV is the \textbf{retail price}. This price is significantly higher than the wholesale price because it includes numerous additional components:
    \begin{itemize}
        \item The wholesale energy cost itself.
        \item \textbf{Grid Fees:} Charges for using the transmission and distribution networks.
        \item \textbf{Taxes and Levies:} National or regional taxes applied to electricity consumption.
        \item \textbf{Supplier Margin:} The profit margin for the retail energy provider.
    \end{itemize}
\end{itemize}
This significant gap between the retail buying price and the wholesale selling price is often referred to as the "retail-wholesale spread." It is this spread that creates the primary opportunity for profitable energy arbitrage. A V2G agent aims to "buy low" (charge the EV when the retail price is lowest, often corresponding to low wholesale prices overnight) and "sell high" (discharge the EV to the grid when the wholesale price is highest, typically during evening peak demand), capturing a portion of this spread as profit. Any successful control strategy must be acutely aware of this quantitative and qualitative difference to make economically rational decisions.

\section{The Electric Vehicle and the V2G Scenario}
\label{sec:ev_and_scenario}
Before delving into the control algorithms, it is useful to establish a clear understanding of the two core components of the problem: the electric vehicle itself, viewed as a controllable asset, and the environment or "scenario" in which it operates. The interaction between these two components defines the boundaries and objectives of any V2G optimization task.

\subsection{Key Characteristics of a Grid-Interactive EV}
From the perspective of the power grid, an electric vehicle is more than just a mode of transport; it is a sophisticated, mobile energy storage device. For the purposes of V2G, an EV can be abstracted by a few key characteristics:
\begin{itemize}
    \item \textbf{The Battery:} This is the heart of the EV as a grid asset. It is defined by its \textbf{energy capacity} (measured in kWh), which determines how much energy it can store, and its \textbf{power limits} (in kW), which dictate the maximum rate at which it can charge or discharge.
    \item \textbf{The On-Board Charger (OBC):} For AC charging (the most common type), the OBC converts alternating current from the grid to direct current to charge the battery. Its power rating is often the bottleneck for charging and V2G power, even if the battery itself could handle more.
    \item \textbf{Communication Interface:} To participate in smart charging or V2G, the EV must be able to communicate with the charging station. This is governed by standards like \textbf{ISO 15118}, which enables the exchange of information required for bidirectional power flow and advanced control.
\end{itemize}
When these characteristics are combined with the vehicle's availability—its arrival and departure times from a charging station—the EV transforms from a simple load into a dispatchable grid resource, capable of both consuming and providing power within a specific set of constraints.

\subsection{Deconstructing the V2G Scenario: A Conceptual Overview}
The environment in which an EV operates is just as important as the vehicle's own characteristics. In this research, these environments are defined by a set of configuration files (e.g., `BusinessPST.yaml`, `V2GProfitMax.yaml`), each representing a distinct, plausible real-world scenario. While the technical details of these files are explored in Chapter 3, their conceptual purpose is to define the "rules of the game" for the control agent. The parameters within can be understood in three main categories:

\begin{itemize}
    \item \textbf{Physical Infrastructure:} This group of parameters defines the physical world. It includes the number and type of charging stations, their maximum power, and the capacity of the local power transformer. For example, a scenario might represent a small office with 10 AC chargers or a large public fast-charging hub with a high-capacity grid connection.
    
    \item \textbf{Market and Environmental Context:} This defines the external conditions the agent must react to. It includes the time-series data for day-ahead electricity prices, which sets the economic incentive for V2G. It can also include data for other loads connected to the same transformer (an "inflexible base load") and local renewable generation, such as a solar panel array on the building's roof. These elements create a dynamic environment where the net power flow must be managed.
    
    \item \textbf{User Demand and Behavior:} This is perhaps the most crucial and stochastic component. These parameters do not define a fixed schedule, but rather the statistical nature of the EV fleet. For instance, a configuration file might specify that EV arrival times follow a normal distribution centered around 9:00 AM and departures around 5:00 PM, with a certain average and standard deviation for the amount of energy each driver needs. This simulates a typical office parking scenario (`BusinessPST.yaml`). In contrast, a `PublicPST.yaml` scenario might use different distributions to represent a public charging station with shorter, more random parking durations throughout the day.
\end{itemize}
By creating different combinations of these elements, the configuration files allow for the construction of a rich and diverse set of testbeds. Scenarios like `V2GProfitMax.yaml` are likely configured to emphasize price volatility to test profit-seeking behavior, while others might be configured with tight transformer limits to test the agent's ability to avoid overloads. Understanding these conceptual roles is key to interpreting the agent's performance across different environments.

\section{A New Paradigm for Control: Reinforcement Learning}
To tackle the V2G challenge, this work turns to Reinforcement Learning (RL), a domain of machine learning focused on how an intelligent agent can learn to make optimal decisions through trial and error. Unlike traditional methods that require a perfect model of the world, RL learns directly from interaction, making it exceptionally robust.

\subsection{The Language of Learning: Markov Decision Processes (MDPs)}
The mathematical bedrock of RL is the \textbf{Markov Decision Process (MDP)}, formally defined by the tuple $(S, A, p, R, \gamma)$. In the V2G context, these elements represent:
\begin{itemize}
    \item $S$: The state (a snapshot of the world: battery levels, electricity price, time).
    \item $A$: The action (the decision: the charging/discharging rate for each EV).
    \item $p(s',r|s,a)$: The environment's response (the probability of transitioning to a new state $s'$ and receiving reward $r$).
    \item $R$: The reward (the feedback signal: profit generated, penalty for user dissatisfaction).
    \item $\gamma$: The discount factor, which balances immediate versus future rewards.
\end{itemize}
This framework rests on the \textbf{Markov Property}, which posits that the future is independent of the past given the present, allowing the agent to make decisions based solely on the current state.

\subsection{Judging the Future: Value Functions and Actor-Critic Architectures}
The agent's goal is to learn a \textbf{policy}, $\pi(a|s)$, which is a strategy for choosing actions. To achieve this, it learns \textbf{value functions} that estimate the long-term value of being in a certain state ($v_{\pi}(s)$) or taking a specific action in a state ($q_{\pi}(s, a)$).
\\
\noindent
The \textbf{Actor-Critic} architecture offers an elegant way to learn this policy. It maintains two distinct neural networks:
\begin{itemize}
    \item \textbf{The Critic}: It learns the value function. Its job is to judge the actor's decisions.
    \item \textbf{The Actor}: It represents the policy itself. Its job is to select actions, using the critic's feedback to refine its strategy over time.
\end{itemize}
This architecture is particularly potent for V2G because it can directly learn a policy over a continuous action space, allowing for fine-grained control of power. The agent's entire behavior, however, is shaped by the reward signal it receives. The complex art of designing this signal to align the agent's goals with our multi-faceted objectives is a critical discipline in itself, known as reward engineering.

% ===================================================================
% REWARD SHAPING SECTION
% ===================================================================
\input{reward}


\section{The Rise of Deep Reinforcement Learning for V2G Control}
Fusing RL with the representational power of deep neural networks gives rise to \textbf{Deep Reinforcement Learning (DRL)}, which currently represents the forefront of V2G control. The evolution of DRL algorithms for V2G has produced a sophisticated and robust toolkit, primarily branching into two main families: off-policy and on-policy methods, each with its own philosophy and trade-offs.

\subsection{Off-Policy Methods: Data-Efficient Learning from Experience}
Off-policy algorithms are distinguished by their ability to learn the optimal policy from data generated by a different, often more exploratory, policy. This decoupling allows them to reuse past experiences stored in a \textit{replay buffer}, making them highly sample-efficient and well-suited for complex problems where real-world interaction is costly.

\paragraph{Deep Deterministic Policy Gradient (DDPG)}
A seminal algorithm that extended the success of Deep Q-Networks (DQN) to continuous action spaces, DDPG was a foundational breakthrough for control problems like V2G\footcite{lillicrap2015continuous}. As an Actor-Critic method, it learns a deterministic policy (the Actor) that maps states to specific actions, guided by a Q-value function (the Critic). However, its practical application is often hampered by training instability and a crippling vulnerability to \textbf{overestimation bias}, where the Critic systematically overestimates Q-values. This error propagates through the learning process, leading the Actor to converge on suboptimal policies\footcite{orfanoudakis2022deep, alfaverh2022optima}.

\paragraph{Twin Delayed DDPG (TD3)}
TD3 was engineered specifically to counteract the instabilities of DDPG\footcite{fujimoto2018addressing}. It introduces three crucial innovations:
\begin{enumerate}
    \item \textbf{Clipped Double Q-Learning:} It learns two independent Critic networks and uses the minimum of their Q-value predictions to calculate the target value. This conservative approach effectively mitigates overestimation bias.
    \item \textbf{Delayed Policy Updates:} The Actor and target networks are updated less frequently than the Critic. This allows the Critic's value estimate to stabilize before the policy is modified, leading to smoother and more reliable training.
    \item \textbf{Target Policy Smoothing:} A small amount of clipped noise is added to the target action, which helps to regularize the learning process and prevent the policy from exploiting narrow peaks in the value function.
\end{enumerate}
These additions make TD3 a much more robust and reliable baseline for V2G tasks\footcite{liu2023optimal, wang2022multi}.

\paragraph{Soft Actor-Critic (SAC)}
SAC stands at the cutting edge of continuous control, offering superior sample efficiency and stability\footcite{haarnoja2019soft}. Its core innovation is the \textbf{maximum entropy framework}. Here, the agent's objective is not just to maximize cumulative reward, but to do so while acting as randomly (stochastically) as possible. This entropy bonus encourages broad exploration, preventing premature convergence to a narrow, suboptimal policy, and improves robustness by teaching the agent to "keep its options open"\footcite{logeshwaran2022comparative}.

\paragraph{Truncated Quantile Critics (TQC)}
TQC tackles overestimation bias from a distributional perspective, presenting a more fundamental solution than TD3\footcite{kuznetsov2020controlling}. Instead of learning a single expected return (a Q-value), it learns the entire \textit{distribution of returns} using quantile regression with multiple Critic networks. Its key mechanism is to "truncate" (discard) the top-k most optimistic quantile estimates when forming the target distribution, thereby systematically removing the primary source of overestimation bias.

\paragraph{Enhancement: Prioritized Experience Replay (PER)}
This is not a standalone algorithm but a crucial modification for off-policy methods. Rather than sampling uniformly from the replay buffer, PER samples transitions with a probability proportional to their "importance," typically measured by the magnitude of their TD error. This focuses the learning process on "surprising" or informative experiences, significantly accelerating convergence\footcite{schaul2015prioritized}.

\subsection{On-Policy Methods: Stability through Cautious Updates}
On-policy methods learn from data generated exclusively by the current policy. This means that after each policy update, all previously collected data must be discarded. While this makes them inherently less sample-efficient, their updates are often more stable and less prone to divergence.

\paragraph{Advantage Actor-Critic (A2C/A3C)}
A2C is a foundational on-policy Actor-Critic algorithm. Its practical and powerful extension, \textbf{Asynchronous Advantage Actor-Critic (A3C)}, employs parallel workers to interact with multiple copies of the environment. These workers update a global set of parameters asynchronously, which decorrelates the data stream and provides a powerful stabilizing effect on the learning process\footcite{mnih2016asynchronous}.

\paragraph{Trust Region Policy Optimization (TRPO)}
TRPO was the first algorithm to rigorously formalize the idea of controlling the policy update size to guarantee stable, monotonic improvements\footcite{schulman2020trust}. It maximizes a surrogate objective function subject to a constraint on the "behavioral change" of the policy, measured by the Kullback-Leibler (KL) divergence. This creates a "trust region" around the old policy, preventing catastrophic updates that could destroy performance. Its implementation, however, is complex as it requires second-order optimization.

\paragraph{Proximal Policy Optimization (PPO)}
PPO achieves the stability benefits of TRPO using only first-order optimization, making it far simpler to implement and more broadly applicable\footcite{schulman2017proximal}. Instead of a hard constraint, PPO modifies the objective function with a \textbf{clipping} mechanism that disincentivizes policy updates resulting in a large probability ratio between the new and old policies. This creates a "soft" trust region and has become a default choice for many on-policy applications due to its robustness and ease of use.

\subsection{Gradient-Free Methods: An Alternative Path}
\paragraph{Augmented Random Search (ARS)}
ARS is an on-policy, gradient-free method that optimizes the policy by operating directly in the parameter space\footcite{mania2018simple}. Instead of calculating gradients, it explores random directions around the current policy parameters and updates them based on the observed performance. While often much less sample-efficient than gradient-based methods for complex V2G problems, its simplicity can make it competitive in certain scenarios.

\section{The Model-Based Benchmark: Model Predictive Control (MPC)}
While DRL offers a powerful model-free approach, it must be benchmarked against its most robust model-based counterpart: \textbf{Model Predictive Control (MPC)}. MPC is not a recent invention; its roots trace back to the 1970s in the chemical engineering and process control industries. Pioneers in the field, such as Richalet et al. with their Model Algorithmic Control (MAC) \footcite{Richalet1978ModelPH} and Cutler and Ramaker with Dynamic Matrix Control (DMC) \footcite{Cutler1980}, developed these techniques to manage complex industrial processes with slow dynamics. The core ideas have since been rigorously formalized and expanded for a vast range of applications by leading researchers like Mayne, Rawlings, and Bemporad, whose work established firm theoretical foundations for stability and optimality \footcite{mayne2000constrained}.

At its heart, MPC is an advanced control method that uses an explicit model of the system to predict its future evolution and compute an optimal control sequence over a finite prediction horizon, $N$. Its primary strength, especially relevant in the V2G context, is the ability to proactively handle complex dynamics and operational constraints \footcite{minchala2025systematic}.

\subsection{Implicit MPC: Online Optimization}
The most common formulation of MPC is \textbf{Implicit MPC}, where a detailed optimization problem is solved online at each control step. For a linear time-invariant system, this problem is typically a Quadratic Program (QP).
\\
\noindent
The controller's objective is to find a sequence of future control inputs $U = [u_{t|t}, ..., u_{t+N-1|t}]$ that minimizes a cost function $J$, which penalizes deviations from a desired state and the control effort itself.

\begin{equation}
\min_{U} J(x_t, U) = \sum_{k=0}^{N-1} (x_{t+k|t}^T Q x_{t+k|t} + u_{t+k|t}^T R u_{t+k|t}) + x_{t+N|t}^T P x_{t+N|t}
\end{equation}
where $x_{t+k|t}$ is the predicted state at future time $k$ based on information at current time $t$, and $Q$, $R$, and $P$ are weighting matrices.

\noindent
This optimization is subject to critical constraints that define the system's valid operating envelope:
\begin{itemize}
    \item \textbf{System Dynamics:} The model that predicts the next state.
    \begin{equation}
    x_{t+k+1|t} = A x_{t+k|t} + B u_{t+k|t}
    \end{equation}
    \item \textbf{State and Input Constraints:} Physical or operational limits.
    \begin{gather}
    x_{min} \le x_{t+k|t} \le x_{max} \\
    u_{min} \le u_{t+k|t} \le u_{max}
    \end{gather}
\end{itemize}
At each time step $t$, this problem is solved to find the optimal control sequence $U^*$. Only the first action, $u_{t|t}^*$, is applied to the system. The entire process is then repeated at the next time step, $t+1$, using new state measurements—a principle known as a \textit{receding horizon}. This constant re-evaluation gives MPC its feedback mechanism and robustness to disturbances.

\subsection{Explicit MPC: Offline Pre-computation}
For systems with fast dynamics or limited online computational power, \textbf{Explicit MPC} offers an alternative. In this paradigm, the optimization problem is solved offline for all possible states within the operating range using multi-parametric programming.
\\
\noindent
The result is a pre-computed, explicit control law, $\pi(x_t)$, which is a \textbf{piecewise affine function} of the state vector $x_t$. The state space is partitioned into a set of distinct polyhedral regions, $\mathcal{X}_i$, each with its own corresponding optimal control law.
\begin{equation}
u^*(x_t) = F_i x_t + g_i \quad \text{if } x_t \in \mathcal{X}_i
\end{equation}
Here, $F_i$ is the gain matrix and $g_i$ is the offset for region $i$. The online operation is reduced to two simple steps:
\begin{enumerate}
    \item Identify which region $\mathcal{X}_i$ the current state $x_t$ belongs to (a fast lookup procedure).
    \item Apply the corresponding pre-computed affine control law.
\end{enumerate}
This eliminates the need for a powerful online solver but comes at the cost of a potentially very high offline computation burden and significant memory requirements to store the lookup table of control laws.

\section{A Comparative Perspective on Control Methodologies}
While DRL represents the cutting edge, it is crucial to contextualize it within the broader landscape.

\begin{table}[h!]
\centering
\caption{Comparative Analysis: DRL vs. Model Predictive Control (MPC) for V2G}
\label{tab:drl_vs_mpc}
\resizebox{\textwidth}{!}{
\begin{tabular}{|p{0.2\linewidth}|p{0.35\linewidth}|p{0.35\linewidth}|}
\hline
\textbf{Aspect} & \textbf{Deep Reinforcement Learning (DRL)} & \textbf{Model Predictive Control (MPC)} \\
\hline
\textbf{Paradigm} & Model-Free, learning-based. Learns optimal policy via trial-and-error. & Model-Based, optimization-based. Solves an optimization problem at each step. \\
\hline
\textbf{Strengths} & \begin{itemize} \item Highly robust to uncertainty and stochasticity. \item No need for an explicit system model. \item Can learn complex, non-linear control policies. \item Fast inference time once trained. \end{itemize} & \begin{itemize} \item Explicitly handles hard constraints (safety guarantees). \item Proactive and anticipatory if forecasts are accurate. \item Well-established and understood. \end{itemize} \\
\hline
\textbf{Weaknesses} & \begin{itemize} \item Can be sample-inefficient during training. \item Lacks hard safety guarantees (an active research area). \item "Black box" nature can make policies hard to interpret. \end{itemize} & \begin{itemize} \item Performance is fundamentally tied to model and forecast accuracy. \item Computationally expensive at each time step (curse of dimensionality). \item Brittle to forecast errors and unmodeled dynamics. \end{itemize} \\
\hline
\textbf{V2G Suitability} & Excellent for dynamic, uncertain environments with complex trade-offs. & Good for problems with simple dynamics and reliable forecasts, but struggles with real-world V2G complexity. \\
\hline
\end{tabular}
}
\end{table}
\noindent
\textbf{Model Predictive Control (MPC)} is the most powerful model-based alternative \footcite{alsabbagh2022reinforcement}. Its primary strength is its ability to handle constraints. However, its performance is fundamentally shackled to the accuracy of its internal model and forecasts \footcite{faggio2023design}. In the V2G domain, creating an accurate model is nearly impossible due to non-linear battery dynamics, market volatility, and human unpredictability. Furthermore, solving the large-scale Mixed-Integer Linear Program (MILP) required at each time step becomes computationally intractable for large fleets \footcite{schwenk2022computationally}.
\\
\noindent
Other methods, such as \textbf{meta-heuristic algorithms} (e.g., genetic algorithms), are typically used for offline scheduling and lack the real-time responsiveness required for dynamic V2G control \footcite{kumar2024integration}.
\\
\noindent
Ultimately, the singular advantage of DRL lies in its native ability to learn and internalize the complex, non-linear trade-offs of the multi-objective V2G problem directly from data. This makes it uniquely suited to navigating the uncertainties of the real world. While other methods have their place, DRL stands out as the most promising technology for deploying the truly intelligent, autonomous, and robust V2G management systems required to achieve the ambitious energy and climate goals of the European Union.

\section{A Primer on Lithium-Ion Battery Chemistries and Degradation}
The effectiveness of any V2G strategy is, at its core, constrained by the physical characteristics of the vehicle's battery. The choice of battery chemistry is not a minor detail; it dictates the operational envelope of the EV, influencing its energy density, power capabilities, lifespan, and, critically, its safety. A clear understanding of these trade-offs is essential to the development of robust and realistic control algorithms.
This section provides an overview of the primary degradation mechanisms, the most prevalent lithium-ion chemistries, and the core concepts governing their performance.

\subsection{Fundamental Concepts and Degradation Mechanisms}
Battery degradation is an irreversible process that reduces a battery's capacity (energy fade) and increases its internal resistance (power fade). It can be broadly categorized into two types: calendar aging and cyclic aging\footcite{birkl2017degradation}.

\begin{itemize}
    \item \textbf{Calendar Aging:} This refers to degradation that occurs whenever the battery is at rest, even when not in use. The primary mechanism is the slow, continuous growth of the \textbf{Solid Electrolyte Interphase (SEI)} layer on the anode surface. The SEI is a necessary passivation layer that forms during the first few cycles, but its continued growth consumes active lithium ions and electrolyte, leading to irreversible capacity loss and increased impedance. The rate of SEI growth is strongly accelerated by two factors:
    \begin{itemize}
        \item \textbf{High Temperature:} Higher temperatures increase the rate of chemical reactions, causing the SEI to grow faster.
        \item \textbf{High State of Charge (SoC):} A high SoC corresponds to a low anode potential, which makes the anode more reactive with the electrolyte, thus promoting SEI growth\footcite{vetter2022ageing}. Storing a battery at 100\% SoC, especially in a hot environment, is one of the most significant contributors to calendar aging.
    \end{itemize}

    \item \textbf{Cyclic Aging:} This degradation occurs as a direct result of charging and discharging the battery. Key mechanisms include:
    \begin{itemize}
        \item \textbf{Mechanical Stress:} During intercalation and de-intercalation, the active materials in the electrodes expand and contract. Over many cycles, this repeated mechanical stress can cause micro-cracks in the electrode particles, leading to a loss of electrical contact and capacity. This effect is more pronounced with larger \textbf{Depths of Discharge (DoD)}.
        \item \textbf{SEI Layer Instability:} The volume changes during cycling can also crack the protective SEI layer, exposing fresh anode material to the electrolyte. This triggers the formation of new SEI, consuming more lithium in the process.
        \item \textbf{Lithium Plating:} Under conditions of high charging rates (high C-rate) and/or low temperatures, lithium ions may not have sufficient time to properly intercalate into the graphite anode. Instead, they deposit on the anode surface as metallic lithium. This is highly detrimental as it causes rapid, irreversible capacity loss and can form needle-like structures called dendrites, which can pierce the separator and cause an internal short circuit, posing a severe safety risk\footcite{birkl2017degradation}.
    \end{itemize}
\end{itemize}
For V2G applications, which inherently involve frequent charge/discharge cycles, understanding and mitigating cyclic aging is paramount.

\subsection{Key Automotive Chemistries}
The EV market is dominated by a few key families of lithium-ion batteries, primarily distinguished by their cathode materials.

\begin{itemize}
    \item \textbf{Lithium Nickel Manganese Cobalt Oxide (NMC):} A highly popular choice due to its balanced performance. By adjusting the ratio of Nickel, Manganese, and Cobalt, manufacturers can tailor the battery to prioritize either energy density (higher Nickel content, e.g., NMC811) or safety and longevity (higher Manganese/Cobalt content, e.g., NMC532).
    \item \textbf{Lithium Nickel Cobalt Aluminum Oxide (NCA):} Similar to NMC but uses Aluminum instead of Manganese. This chemistry, famously used by Tesla for many years, offers very high energy density, enabling longer ranges, but at the cost of slightly lower cycle life and safety margins compared to NMC.
    \item \textbf{Lithium Iron Phosphate (LFP):} This chemistry is rapidly gaining market share. It contains no cobalt, making it cheaper and more ethically sourced. LFP batteries offer exceptional cycle life and are considered the safest among common Li-ion types. Their main drawbacks are lower nominal voltage and lower energy density.
    \item \textbf{Lithium Titanate Oxide (LTO):} LTO batteries use a titanate anode. They are exceptional in terms of safety, cycle life (>10,000 cycles), and low-temperature performance. However, their very low energy density and high cost make them a niche solution.
\end{itemize}

\subsection{Voltage Profiles and the Challenge of SoC Estimation}
The relationship between a battery's voltage and its SoC is a critical, non-linear function. The derivative of the cell voltage with respect to the DoD, $\frac{dV_{cell}}{d(DoD)}$, is a crucial parameter for the Battery Management System (BMS). A steep, consistent slope allows the BMS to accurately infer the SoC from a voltage measurement. Conversely, a flat slope ($\frac{dV_{cell}}{d(DoD)} \approx 0$) makes this estimation extremely difficult, as a small voltage measurement error can translate into a massive SoC error.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title={Discharge Voltage Curves of Li-ion Chemistries},
            xlabel={Depth of Discharge (DoD) [\%]},
            ylabel={Cell Voltage (V)},
            xmin=0, xmax=100,
            ymin=2.0, ymax=4.5,
            grid=major,
            legend pos=south west,
            width=\textwidth,
            height=7.5cm,
        ]
        \addplot[smooth, thick, red] coordinates { (0, 4.2) (10, 4.0) (20, 3.8) (50, 3.6) (80, 3.4) (95, 3.2) (100, 2.5) };
        \addlegendentry{NCA/NMC811}

        \addplot[smooth, thick, blue] coordinates { (0, 4.2) (10, 4.05) (20, 3.9) (50, 3.7) (80, 3.5) (95, 3.3) (100, 2.7) };
        \addlegendentry{NMC}

        \addplot[smooth, thick, green!60!black] coordinates { (0, 3.65) (5, 3.4) (10, 3.3) (20, 3.25) (80, 3.2) (90, 3.0) (95, 2.8) (100, 2.5) };
        \addlegendentry{LFP}
        
        \addplot[smooth, thick, orange] coordinates { (0, 2.8) (10, 2.5) (20, 2.4) (80, 2.2) (90, 2.0) (100, 1.8) };
        \addlegendentry{LTO}

        \draw[dashed, thick, gray] (axis cs:15,3.28) -- (axis cs:85,3.22);
        \node[pin=135:{\parbox{2.5cm}{\centering \small LFP Voltage Plateau: \\ $\frac{dV_{cell}}{d(DoD)} \approx 0$ \\ Difficult SoC Estimation}}] at (axis cs:50,3.25) {};
        \end{axis}
    \end{tikzpicture}
    \caption{Typical discharge voltage curves for various lithium-ion chemistries. The flat profile of LFP makes accurate SoC estimation challenging based on voltage alone\footcite{plett2015battery}.}
    \label{fig:voltage_curves_detailed}
\end{figure}

As shown in Figure \ref{fig:voltage_curves_detailed}, LFP's flat voltage plateau makes it difficult for the BMS to determine the precise SoC in the central part of its operating range. This necessitates periodic full charges to 100\% to recalibrate the system at a point where the voltage curve is steep again, an important operational constraint for V2G control strategies.

\subsection{Comparative Analysis and Safety Considerations}
The trade-offs between chemistries are summarized in Table \ref{tab:chem_comparison_detailed}. Safety is paramount, and the primary risk is thermal runaway. The risk is directly related to the stored energy density ($\Delta E / \Delta m$). A higher energy density means more energy is packed into a smaller mass, which can be released violently if the cell's structure is compromised. Consequently, the critical temperature for initiating thermal runaway is generally lower for higher energy density chemistries. As energy density decreases, the thermal stability increases.
\begin{table}[h!]
\centering
\small % Reduces the font size for the entire table
\caption{Comparative analysis of key automotive battery chemistries.}
\label{tab:chem_comparison_detailed}
% \textwidth makes the table span the full text width.
% The first column is left-aligned and bold.
% The other 5 columns are centered and will wrap text if needed.
\begin{tabularx}{\textwidth}{
  @{} % Removes padding on the left
  >{\bfseries\RaggedRight}X % First column: bold, left-aligned, wrapping
  *{5}{>{\Centering\arraybackslash}X} % Next 5 columns: centered, wrapping
  @{} % Removes padding on the right
}
\toprule
Metric & NCA & NMC & LFP & LTO & LCO \\
\midrule
Energy Density (Wh/kg) & 200 - 260 (Highest) & 150 - 220 (High) & 90 - 160 (Moderate) & 60 - 110 (Low) & 150-200 (High) \\
\addlinespace % Adds a little vertical space for readability
Cycle Life & 1000 - 2000 & 1000 - 2500 & 2000 - 5000+ & >10,000 & 500 - 1000 \\
\addlinespace
Safety & Good & Very Good & Excellent & Excellent & Poor \\
\addlinespace
Thermal Runaway Temp ($^{\circ}$C) & $\sim$150 - 180 & $\sim$180 - 210 & $\sim$220 - 270 & >250 & $\sim$150 \\
\bottomrule
\end{tabularx}
\end{table}