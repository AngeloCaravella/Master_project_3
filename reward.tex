\subsection{Advanced Reward Engineering}
Within a Reinforcement Learning (RL) framework, the architecture of the reward function becomes the linchpin of success. 
A simplistic formulation say, a single reward term tied to short-term profits, inevitably produces short-sighted and even harmful strategies. 
\\
\noindent
By contrast, a carefully crafted reward must reflect the multi-dimensional landscape: penalties for excessive cycling, bonuses for aligning with user preferences, and incentives for contributing to grid stability. 
\\
\noindent
Recent research has gone a step further by introducing \textbf{adaptive reward shaping}. 
Instead of relying on static weights for each component of the reward, these approaches allow the structure or the weights to evolve dynamically during training. 
One possible scheme begins by emphasizing profit, which enables the agent to quickly acquire the basics of arbitrage. 
Once a plateau in performance is reached, the system progressively increases the penalty terms associated with battery wear or unmet user mobility targets. 
This staged adjustment steers the agent away from narrow, short-term gains and towards policies that remain robust in the long run, ultimately producing strategies that balance profitability, reliability, and sustainability \footcite{wan2022dynamic}.



\subsubsection{A Taxonomy of Reward Shaping Techniques}
Reward shaping refers to the practice of enriching an environment’s original reward signal,often sparse, delayed, or difficult to interpret—with additional terms that accelerate learning and provide intermediate guidance to the agent. Over the years, several methodologies have emerged, each grounded in different theoretical principles and suited to different problem settings. What follows is a taxonomy of the most relevant approaches in the context of V2G control. 

\begin{description}
    \item[Potential-Based Reward Shaping (PBRS)] 
    \noindent
    Perhaps the most theoretically rigorous approach, PBRS was introduced by Ng et al.\ and has the remarkable property of guaranteeing policy invariance: the optimal policy of the original Markov Decision Process is preserved, while convergence can be significantly accelerated \footcite{ng1999policy}. 
    The modified reward $R'$ is obtained as:
    \begin{equation}
        R'(s, a, s') = R(s, a, s') + F(s, s') = R(s, a, s') + \gamma \Phi(s') - \Phi(s)
    \end{equation}
    where $\Phi: S \to \mathbb{R}$ is a potential function defined over the state space and $\gamma$ the discount factor. 
    Intuitively, the shaping term $F$ rewards transitions that increase the potential $\Phi$. 
    In a V2G setting, $\Phi(s)$ might assign higher values as the aggregate SoC of connected EVs approaches their target levels, thus supplying dense feedback for incremental progress without altering the long-term objective. 

    \item[Dynamic and Adaptive Reward Shaping] 
    \noindent
    In contrast to PBRS, which prioritizes theoretical guarantees, adaptive approaches deliberately relax policy invariance to address the intricacies of multi-objective control. 
    Here, the reward function itself evolves during training, either in response to the state of the system or according to a predefined schedule:
    \begin{itemize}
        \item \textbf{State-Dependent Shaping:} Reward weights adapt to the current state $s_t$. For example, the penalty associated with transformer overloading can be defined as $\lambda^{\text{tr}}(s_t)$, increasing exponentially as the load approaches a critical threshold. In this way, constraint violations are emphasized precisely when they become imminent. 
        \item \textbf{Time- or Schedule-Based Shaping:} The relative importance of different reward components is varied across training episodes. An agent may initially be exposed to a reward function dominated by profitability, before progressively incorporating penalties for user dissatisfaction and battery degradation. This staged modification closely mirrors the logic of curriculum-based training. 
    \end{itemize}
    Such adaptive methods are particularly well-suited for scenarios, like V2G, where the notion of an “optimal” trade-off among competing objectives must itself be discovered rather than imposed from the outset. 

    \item[Curriculum Learning (CL)] 
    \noindent
    Although strictly speaking a training paradigm rather than a reward shaping method, CL can be interpreted as an implicit form of shaping, since it gradually modifies both the environment and the associated reward structure. 
    The agent is not immediately confronted with the full problem complexity, but instead progresses through a sequence of tasks of increasing difficulty. 
    A possible curriculum for V2G might include:
    \begin{enumerate}
        \item \textbf{Phase 1:} Single EV, deterministic price signals, reward based solely on arbitrage profit. 
        \item \textbf{Phase 2:} Multiple EVs, stochastic pricing, introduction of user satisfaction penalties. 
        \item \textbf{Phase 3:} Full-scale environment including grid constraints, degradation costs, and the complete adaptive reward function. 
    \end{enumerate}
    This progression enables the agent to acquire foundational skills before addressing the most challenging aspects of the task, ultimately leading to more robust and transferable policies. 
\end{description}


\subsubsection{Most Utilized Techniques in Reinforcement Learning for V2G}
In the specific context of Vehicle-to-Grid management, the most effective and commonly used techniques are \textbf{Dynamic and Adaptive Reward Shaping} and \textbf{Curriculum Learning}.
\noindent
\\
The reason for their prevalence is rooted in the nature of the V2G problem itself. It is a multi-objective optimization problem with deeply intertwined and often conflicting goals (e.g., maximizing profit vs. minimizing battery wear, ensuring grid stability vs. guaranteeing user satisfaction).

\begin{itemize}
    \item \textbf{Dynamic/Adaptive Reward Shaping} is exceptionally well-suited for V2G because the relative importance of each objective is not static; it is state-dependent. For example, satisfying a user's charging request is of little importance when they have 10 hours left, but it becomes critically important when they have 10 minutes left. An adaptive reward function that calculates an "urgency score" can capture this dynamic priority, which is impossible with a fixed-weight penalty. This allows the agent to learn a far more nuanced and realistic control policy.
    
    \item \textbf{Curriculum Learning} is widely used as a practical strategy to make the training of complex DRL agents for V2G feasible. Training an agent on the full, stochastic, multi-objective V2G problem from scratch is often unstable and inefficient. By using a curriculum, the agent can first master basic concepts (like energy arbitrage) before moving on to handle complex constraints (like transformer limits and user deadlines), leading to more stable and effective final policies.
\end{itemize}
\noindent
Conversely, \textbf{Potential-Based Reward Shaping (PBRS)} is less utilized for the overall V2G control problem. Its core strength—policy invariance—is actually a limitation here. The goal in V2G is not to find the optimal policy for a simple, predefined objective (like pure profit), but rather to discover a novel policy that represents the \textit{best possible compromise} between all objectives. Dynamic shaping intentionally alters the learning objective to guide the agent to this superior compromise, a task for which PBRS is not designed.
\\
To further refine this balance, recent research has explored \textbf{adaptive reward shaping}. Instead of using fixed weights for different components of the reward function, these methods dynamically adjust the weights or the structure of the reward during training. For example, an agent might initially be incentivized primarily by profit to learn the basic mechanics of arbitrage. As its performance plateaus, the penalty for battery degradation or for failing to meet user departure targets can be gradually increased. This guides the agent toward a more holistic and robust final policy, preventing a premature convergence to a suboptimal strategy that ignores long-term costs like battery health \footcite{wan2022dynamic}.